# -*- coding: utf-8 -*-
"""scrapping and preprocessing r.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LcJzqWbja-qXCLq26rHUK1mPp4Nyqfen
"""

#package
!pip install bs4

from bs4  import BeautifulSoup as bs
#request to access the content
import requests
# writing in csv form
from csv import writer
import pandas as pd

links=('https://www.nhs.uk/conditions/kidney-disease/',
     'https://www.mayoclinic.org/diseases-conditions/chronic-kidney-disease/symptoms-causes/syc-20354521')
with open('kidney.csv','w',encoding='utf8',newline='') as f:
  thewriter=writer(f)   
  header=['websiteS'] 
  thewriter.writerow(header)  
  for link in links:
    response = requests.get(link)
    html = response.content
    soup = bs(html,'html.parser')
    paragraphs = soup.find_all('p')
    for p in paragraphs:
      para = p.text
      info=[para]
      thewriter.writerow(info)

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv('/content/kidney.csv')
df

df=df.dropna(how='all')
df

#text preprocessing
import nltk                               
import matplotlib.pyplot as plt           
import random  
import pandas as pd

#Removing punctuation
import string
string.punctuation
#Defining Function to remove punctuation
def remove_punctuation(text):
    punctuationfree="".join([i for i in text if i not in string.punctuation])
    return punctuationfree

#Storing the puntuation free text
df['clean']= df['websiteS'].apply(lambda x:remove_punctuation(x))
display(df.head(3))
#Lowering the tweets
df['lower']= df['clean'].apply(lambda x: x.lower())

#Tokenization
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import TweetTokenizer as tt
#applying function to the column
tokenizer = tt()      
df['tokenized'] = df['lower'].apply(lambda x: tokenizer.tokenize(x))

#Removing stop words
from nltk.corpus import stopwords
nltk.download('stopwords')
stopword = stopwords.words('english')
#Defining the function to remove stopwords from tokenized text
def remove_stopwords(text):
    output= [i for i in text if i not in stopword]
    return output
#applying the function
df['stopwords_remove']= df['tokenized'].apply(lambda x:remove_stopwords(x))

#Stemming 
from nltk.stem.porter import PorterStemmer
porter_stemmer = PorterStemmer()
#Defining a function for stemming
def stemming(text):
    stem_text = [porter_stemmer.stem(word) for word in text]
    return stem_text
df['stemmed']=df['stopwords_remove'].apply(lambda x: stemming(x))

#Lemmatization
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
nltk.download('omw-1.4')
wordnet_lemmatizer = WordNetLemmatizer()
#Defining the function for lemmatization
def lemmatizer(text):
    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]
    return lemm_text
df['lemmatized']=df['stemmed'].apply(lambda x:lemmatizer(x))
df.head(5)